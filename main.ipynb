{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"F:\\Thesis\\code\\final run\\Data\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(dataset_path + \"/spiral/training\", transform=transform)\n",
    "test_dataset = ImageFolder(dataset_path + \"/spiral/testing\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(train_dataset) * 0.83)\n",
    "dev_size = len(train_dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, dev_dataset = random_split(train_dataset, [train_size, dev_size])\n",
    "test_dataset = ImageFolder(dataset_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_size = len(train_dataset)\n",
    "total_test_size = len(test_dataset)\n",
    "total_dev_size = len(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 2\n",
    "input_dim = 784\n",
    "\n",
    "num_clients = 8\n",
    "rounds = 30\n",
    "batch_size = 128\n",
    "epochs_per_client = 3\n",
    "learning_rate = 2e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader(DataLoader):\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            yield to_device(batch, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 20, 7)\n",
    "        self.conv2 = torch.nn.Conv2d(20, 40, 7)\n",
    "        self.maxpool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear = torch.nn.Linear(2560, 2)\n",
    "        self.non_linearity = torch.nn.functional.relu\n",
    "        self.track_layers = {'conv1': self.conv1, 'conv2': self.conv2, 'linear': self.linear}\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        out = self.conv1(x_batch)\n",
    "        out = self.non_linearity(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.non_linearity(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "    def get_track_layers(self):\n",
    "        return self.track_layers\n",
    "\n",
    "    def apply_parameters(self, parameters_dict):\n",
    "        with torch.no_grad():\n",
    "            for layer_name in parameters_dict:\n",
    "                self.track_layers[layer_name].weight.data *= 0\n",
    "                self.track_layers[layer_name].bias.data *= 0\n",
    "                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n",
    "                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n",
    "\n",
    "    def get_parameters(self):\n",
    "        parameters_dict = dict()\n",
    "        for layer_name in self.track_layers:\n",
    "            parameters_dict[layer_name] = {\n",
    "                'weight': self.track_layers[layer_name].weight.data,\n",
    "                'bias': self.track_layers[layer_name].bias.data\n",
    "            }\n",
    "        return parameters_dict\n",
    "\n",
    "    def batch_accuracy(self, outputs, labels):\n",
    "        with torch.no_grad():\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "\n",
    "    def _process_batch(self, batch):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        accuracy = self.batch_accuracy(outputs, labels)\n",
    "        return (loss, accuracy)\n",
    "\n",
    "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
    "        optimizer = opt(self.parameters(), lr)\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss.detach()\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            avg_loss = torch.stack(losses).mean().item()\n",
    "            avg_acc = torch.stack(accs).mean().item()\n",
    "            history.append((avg_loss, avg_acc))\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        return (avg_loss, avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_client=[]\n",
    "class Client:\n",
    "    def __init__(self, client_id, dataset):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "\n",
    "    def train(self, parameters_dict):\n",
    "        net = to_device(FederatedNet(), device)\n",
    "        net.apply_parameters(parameters_dict)\n",
    "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
    "        history_client.append(train_history)\n",
    "        print('{}: Loss = {}, Accuracy = {}'.format(self.client_id, round(train_history[-1][0], 4), round(train_history[-1][1], 4)))\n",
    "        return net.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_client = total_train_size // num_clients\n",
    "client_datasets = random_split(train_dataset, [min(i + examples_per_client,\n",
    "           total_train_size) - i for i in range(0, total_train_size, examples_per_client)])\n",
    "clients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = to_device(FederatedNet(), device)\n",
    "history = []\n",
    "history_client=[]\n",
    "for i in range(rounds):\n",
    "    print('Start Round {} ...'.format(i + 1))\n",
    "    curr_parameters = global_net.get_parameters()\n",
    "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
    "    for client in clients:\n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size() / total_train_size\n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
    "            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
    "    global_net.apply_parameters(new_parameters)\n",
    "\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(dev_dataset)\n",
    "    plt.plot([i + 1 for i in range(len(history_client))], [history_client[i][0] for i in range(len(history_client))], color='r', label='client_acc')\n",
    "    plt.plot([i + 1 for i in range(len(history_client))], [history_client[i][1] for i in range(len(history_client))], color='b', label='client_loss')\n",
    "    plt.legend()\n",
    "    plt.title('Client Training history')\n",
    "    plt.show()\n",
    "    print('After round {}, train_acc = {}, train_loss = {}, dev_loss = {}, dev_acc = {}\\n'.format(i + 1, round(train_acc,4),round(train_loss, 4), round(dev_loss, 4),round(dev_acc,4)))\n",
    "    history.append((train_loss, dev_loss,train_acc,dev_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history))], [history[i][0] for i in range(len(history))], color='r', label='train loss')\n",
    "\n",
    "plt.plot([i + 1 for i in range(len(history))], [history[i][2] for i in range(len(history))], color='g', label='train acc')\n",
    "plt.legend()\n",
    "plt.title('Training history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history))], [history[i][1] for i in range(len(history))], color='b', label='dev loss')\n",
    "plt.plot([i + 1 for i in range(len(history))], [history[i][3] for i in range(len(history))], color='y', label='dev acc')\n",
    "plt.legend()\n",
    "plt.title('Training history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_client = total_test_size // num_clients\n",
    "client_datasets = random_split(test_dataset, [min(i + examples_per_client,\n",
    "           total_test_size) - i for i in range(0, total_test_size, examples_per_client)])\n",
    "clients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_client=[]\n",
    "class Client:\n",
    "    def __init__(self, client_id, dataset):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "\n",
    "    def train(self, parameters_dict):\n",
    "        net = to_device(FederatedNet(), device)\n",
    "        net.apply_parameters(parameters_dict)\n",
    "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
    "        history_client.append(train_history)\n",
    "        print('{}: Loss = {}, Accuracy = {}'.format(self.client_id, round(train_history[-1][0], 4), round(train_history[-1][1], 4)))\n",
    "        return net.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = to_device(FederatedNet(), device)\n",
    "history = []\n",
    "history_client=[]\n",
    "for i in range(rounds):\n",
    "    print('Start Round {} ...'.format(i + 1))\n",
    "    curr_parameters = global_net.get_parameters()\n",
    "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
    "    for client in clients:\n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size() / total_test_size\n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
    "            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
    "    global_net.apply_parameters(new_parameters)\n",
    "\n",
    "    test_loss, test_acc = global_net.evaluate(test_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(dev_dataset)\n",
    "    print('After round {}, test_acc = {}, test_loss = {}, dev_loss = {}, dev_acc = {}\\n'.format(i + 1, round(test_acc,4),round(test_loss, 4), round(dev_loss, 4),round(dev_acc,4)))\n",
    "    history.append((test_loss, dev_loss,test_acc,dev_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history))], [history[i][3] for i in range(len(history))], color='y', label='dev acc')\n",
    "\n",
    "plt.plot([i + 1 for i in range(len(history))], [history[i][2] for i in range(len(history))], color='g', label='test acc')\n",
    "plt.legend()\n",
    "plt.title('Testing history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
